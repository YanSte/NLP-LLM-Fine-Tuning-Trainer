{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7311218,"sourceType":"datasetVersion","datasetId":120316}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# | NLP | LLM | Fine-tuning | Trainer |\n\n## Natural Language Processing (NLP) and Large Language Models (LLM) with Fine-Tuning LLM and Trainer\n\n![Learning](https://t3.ftcdn.net/jpg/06/14/01/52/360_F_614015247_EWZHvC6AAOsaIOepakhyJvMqUu5tpLfY.jpg)\n\n\n# <b>1 <span style='color:#78D118'>|</span> Overview</b>\n\nIn this notebook we're going to Fine-Tuning LLM:\n\n<img src=\"https://github.com/YanSte/NLP-LLM-Fine-tuning-Trainer/blob/main/img_2.png?raw=true\" alt=\"Learning\" width=\"50%\">\n\nMany LLMs are general purpose models trained on a broad range of data and use cases. This enables them to perform well in a variety of applications, as shown in previous modules. It is not uncommon though to find situations where applying a general purpose model performs unacceptably for specific dataset or use case. This often does not mean that the general purpose model is unusable. Perhaps, with some new data and additional training the model could be improved, or fine-tuned, such that it produces acceptable results for the specific use case.\n\n<img src=\"https://github.com/YanSte/NLP-LLM-Fine-tuning-Trainer/blob/main/img_1.png?raw=true\" alt=\"Learning\" width=\"50%\">\n\nFine-tuning uses a pre-trained model as a base and continues to train it with a new, task targeted dataset. Conceptually, fine-tuning leverages that which has already been learned by a model and aims to focus its learnings further for a specific task.\n\nIt is important to recognize that fine-tuning is model training. The training process remains a resource intensive, and time consuming effort. Albeit fine-tuning training time is greatly shortened as a result of having started from a pre-trained model. \n\n<img src=\"https://github.com/YanSte/NLP-LLM-Fine-tuning-Trainer/blob/main/img_3.png?raw=true\" alt=\"Learning\" width=\"50%\">\n\n\n## Learning Objectives\n\n By the end of this notebook, you will be able to:\n1. Prepare a novel dataset\n2. Fine-tune the `t5-small` model to classify movie reviews.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"### Setup\n","metadata":{}},{"cell_type":"code","source":"import os\n\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"Fill\"","metadata":{"execution":{"iopub.status.busy":"2024-01-04T20:58:38.244344Z","iopub.execute_input":"2024-01-04T20:58:38.244794Z","iopub.status.idle":"2024-01-04T20:58:38.249922Z","shell.execute_reply.started":"2024-01-04T20:58:38.244756Z","shell.execute_reply":"2024-01-04T20:58:38.248988Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"%%capture\n!py-cpuinfo==9.0.0\n#!pip install chromadb==0.4.10 tiktoken==0.3.3 sqlalchemy==2.0.15\n#!pip install langchain==0.0.249\n!pip install --force-reinstall pydantic==1.10.6 \n#!pip install sentence_transformers","metadata":{"execution":{"iopub.status.busy":"2024-01-04T20:58:38.252224Z","iopub.execute_input":"2024-01-04T20:58:38.252567Z","iopub.status.idle":"2024-01-04T20:58:58.770261Z","shell.execute_reply.started":"2024-01-04T20:58:38.252530Z","shell.execute_reply":"2024-01-04T20:58:58.768943Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport transformers as tr\nfrom datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2024-01-04T20:58:58.772003Z","iopub.execute_input":"2024-01-04T20:58:58.772802Z","iopub.status.idle":"2024-01-04T20:59:01.143985Z","shell.execute_reply.started":"2024-01-04T20:58:58.772762Z","shell.execute_reply":"2024-01-04T20:59:01.143049Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"cache_dir = \"./cache\"","metadata":{"execution":{"iopub.status.busy":"2024-01-04T20:59:01.145302Z","iopub.execute_input":"2024-01-04T20:59:01.145760Z","iopub.status.idle":"2024-01-04T20:59:01.149979Z","shell.execute_reply.started":"2024-01-04T20:59:01.145732Z","shell.execute_reply":"2024-01-04T20:59:01.149048Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_column', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_seq_items', None)\npd.set_option('display.max_colwidth', 500)\npd.set_option('expand_frame_repr', True)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T20:59:01.153102Z","iopub.execute_input":"2024-01-04T20:59:01.153940Z","iopub.status.idle":"2024-01-04T20:59:01.161303Z","shell.execute_reply.started":"2024-01-04T20:59:01.153913Z","shell.execute_reply":"2024-01-04T20:59:01.160430Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Creating a local temporary directory on the Driver. \n# This will serve as a root directory for the intermediate model checkpoints created during the training process. The final model will be persisted to DBFS.\nimport tempfile\n\ntmpdir = tempfile.TemporaryDirectory()\nlocal_training_root = tmpdir.name","metadata":{"execution":{"iopub.status.busy":"2024-01-04T20:59:01.162381Z","iopub.execute_input":"2024-01-04T20:59:01.162726Z","iopub.status.idle":"2024-01-04T20:59:01.171957Z","shell.execute_reply.started":"2024-01-04T20:59:01.162693Z","shell.execute_reply":"2024-01-04T20:59:01.171049Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# <b>2 <span style='color:#78D118'>|</span> Fine-Tuning</b>\n\n### Step 1 - Data Preparation\n\nThe first step of the fine-tuning process is to identify a specific task and supporting dataset. In this notebook, we will consider the specific task to be classifying movie reviews. This idea is generally simple task where a movie review is provided as plain-text and we would like to determine whether or not the review was positive or negative.\n\nThe [IMDB dataset](https://huggingface.co/datasets/imdb) can be leveraged as a supporting dataset for this task. The dataset conveniently provides both a training and testing dataset with labeled binary sentiments, as well as a dataset of unlabeled data.\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-29T10:22:36.998626Z","iopub.execute_input":"2023-12-29T10:22:36.99952Z","iopub.status.idle":"2023-12-29T10:22:37.004082Z","shell.execute_reply.started":"2023-12-29T10:22:36.999482Z","shell.execute_reply":"2023-12-29T10:22:37.002916Z"}}},{"cell_type":"code","source":"imdb_ds = load_dataset(\"imdb\")","metadata":{"execution":{"iopub.status.busy":"2024-01-04T20:59:01.173249Z","iopub.execute_input":"2024-01-04T20:59:01.173871Z","iopub.status.idle":"2024-01-04T20:59:54.518865Z","shell.execute_reply.started":"2024-01-04T20:59:01.173837Z","shell.execute_reply":"2024-01-04T20:59:54.517952Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9af74ef3919b48fb9d7640b71a820b28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1ef57b401774997b9421fc52eb34d11"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a8c06fc86494610b6dd4a6bf121848b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ab4f5ce37d5462fa03f1a97b62a351b"}},"metadata":{}}]},{"cell_type":"code","source":"imdb_ds","metadata":{"execution":{"iopub.status.busy":"2024-01-04T20:59:54.520036Z","iopub.execute_input":"2024-01-04T20:59:54.520315Z","iopub.status.idle":"2024-01-04T20:59:54.527040Z","shell.execute_reply.started":"2024-01-04T20:59:54.520288Z","shell.execute_reply":"2024-01-04T20:59:54.526068Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    unsupervised: Dataset({\n        features: ['text', 'label'],\n        num_rows: 50000\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"### Step 2 - Select pre-trained model\n\n\nThe next step of the fine-tuning process is to select a pre-trained model. We will consider using the [T5](https://huggingface.co/docs/transformers/model_doc/t5) [[paper]](https://arxiv.org/pdf/1910.10683.pdf) family of models for our fine-tuning purposes. The T5 models are text-to-text transformers that have been trained on a multi-task mixture of unsupervised and supervised tasks. They are well suited for tasks such as summarization, translation, text classification, question answering, and more.\n\nThe `t5-small` version of the T5 models has 60 million parameters. This slimmed down version will be sufficient for our purposes.\n","metadata":{}},{"cell_type":"code","source":"model_checkpoint = \"t5-small\"","metadata":{"execution":{"iopub.status.busy":"2024-01-04T20:59:54.528042Z","iopub.execute_input":"2024-01-04T20:59:54.528301Z","iopub.status.idle":"2024-01-04T20:59:54.535380Z","shell.execute_reply.started":"2024-01-04T20:59:54.528277Z","shell.execute_reply":"2024-01-04T20:59:54.534265Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Hugging Face provides the [Auto*](https://huggingface.co/docs/transformers/model_doc/auto) suite of objects to conveniently instantiate the various components associated with a pre-trained model. Here, we use the [AutoTokenizer](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoTokenizer) to load in the tokenizer that is associated with the `t5-small` model.","metadata":{}},{"cell_type":"code","source":"# load the tokenizer that was used for the t5-small model\ntokenizer = tr.AutoTokenizer.from_pretrained(\n    model_checkpoint, \n    cache_dir=cache_dir\n)  # Use a pre-cached model","metadata":{"execution":{"iopub.status.busy":"2024-01-04T20:59:54.536498Z","iopub.execute_input":"2024-01-04T20:59:54.536775Z","iopub.status.idle":"2024-01-04T20:59:58.673116Z","shell.execute_reply.started":"2024-01-04T20:59:54.536745Z","shell.execute_reply":"2024-01-04T20:59:58.672321Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30f09d02813e480a95cf35e69f2a0f76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca685611240c4d38a190318c1b05378f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6e612866f6c4e2dab7f7a9ba4873909"}},"metadata":{}}]},{"cell_type":"markdown","source":"The IMDB dataset is a binary sentiment dataset. Its labels therefore are encoded as (-1 - unknown; 0 - negative; 1 - positive) values. In order to use this dataset with a text-to-text model like T5, the label set needs to be represented as a string. There are a number of ways to accomplish this. Here, we will simply translate each label id to its corresponding string value.","metadata":{}},{"cell_type":"code","source":"import torch\n\ndef to_tokens(tokenizer, label_map):\n    \"\"\"\n    Given a `tokenizer` this closure will iterate through `x` and return the result of `apply()`.\n    This function is mapped to a dataset and returned with ids and attention mask.\n    \"\"\"\n    def apply(x):\n        \"\"\"From a formatted dataset `x` a batch encoding `token_res` is created.\"\"\"\n        target_labels = [label_map[y] for y in x[\"label\"]]\n        token_res = tokenizer(\n            x[\"text\"],\n            text_target=target_labels,\n            return_tensors=\"pt\",\n            truncation=True,\n            padding=True,\n        )\n        # Convert tensors to lists or numpy arrays\n        for key, value in token_res.items():\n            if isinstance(value, torch.Tensor):\n                token_res[key] = value.tolist()\n        return token_res\n    return apply\n\nimdb_label_lookup = {0: \"negative\", 1: \"positive\", -1: \"unknown\"}\n# Assuming tokenizer is defined somewhere before this code\nimdb_to_tokens = to_tokens(tokenizer, imdb_label_lookup)\ntokenized_dataset = imdb_ds.map(\n    imdb_to_tokens,\n    batched=True, # batched=True, it expects the function to return a dictionary of types like (<class 'list'>, <class 'numpy.ndarray'>).\n    remove_columns=[\"text\", \"label\"]\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:29:09.834879Z","iopub.execute_input":"2024-01-04T21:29:09.835854Z","iopub.status.idle":"2024-01-04T21:30:49.117712Z","shell.execute_reply.started":"2024-01-04T21:29:09.835819Z","shell.execute_reply":"2024-01-04T21:30:49.116875Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c8557f5cfee4093a4705ea6b1839539"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb294c25036c41fab4968cab02e640b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b36a3f5f818f40daa7c75f3bf1703d4f"}},"metadata":{}}]},{"cell_type":"code","source":"def test_tokenized_dataset(tokenized_dataset, num_samples=1):\n    # Print the first few samples from the tokenized dataset\n    for i in range(num_samples):\n        sample = tokenized_dataset[i]\n        print(f\"Sample {i + 1}:\")\n        print(\"Input IDs:\", sample[\"input_ids\"])\n        print(\"Attention Mask:\", sample[\"attention_mask\"])\n        print(\"Labels:\", sample[\"labels\"])\n        print(\"=\" * 50)\n\n# Assuming tokenized_dataset is already defined\ntest_tokenized_dataset(tokenized_dataset['train'])","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:34:47.903200Z","iopub.execute_input":"2024-01-04T21:34:47.904149Z","iopub.status.idle":"2024-01-04T21:34:47.912779Z","shell.execute_reply.started":"2024-01-04T21:34:47.904112Z","shell.execute_reply":"2024-01-04T21:34:47.911810Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Sample 1:\nInput IDs: [27, 3, 20907, 27, 5422, 205, 22182, 17854, 18, 476, 3577, 20573, 45, 82, 671, 1078, 250, 13, 66, 8, 21760, 24, 3, 8623, 34, 116, 34, 47, 166, 1883, 16, 18148, 5, 27, 92, 1943, 24, 44, 166, 34, 47, 3, 27217, 57, 412, 5, 134, 5, 1653, 7, 3, 99, 34, 664, 1971, 12, 2058, 48, 684, 6, 2459, 271, 3, 9, 1819, 13, 4852, 1702, 96, 23862, 2660, 23, 138, 121, 27, 310, 141, 12, 217, 48, 21, 1512, 5, 2, 115, 52, 3, 87, 3155, 2, 115, 52, 3, 87, 3155, 634, 5944, 19, 3, 12809, 300, 3, 9, 1021, 16531, 6616, 1236, 2650, 312, 29, 9, 113, 2746, 12, 669, 762, 255, 54, 81, 280, 5, 86, 1090, 255, 2746, 12, 992, 160, 1388, 7, 12, 492, 128, 1843, 13, 12481, 30, 125, 8, 1348, 7320, 15, 221, 816, 81, 824, 1827, 807, 224, 38, 8, 8940, 1602, 11, 1964, 807, 16, 8, 907, 1323, 5, 86, 344, 3558, 13446, 11, 9495, 177, 23, 1847, 7, 13, 23964, 81, 70, 8479, 30, 6525, 6, 255, 65, 3, 7, 994, 28, 160, 6616, 3145, 6, 28345, 6, 11, 4464, 1076, 5, 2, 115, 52, 3, 87, 3155, 2, 115, 52, 3, 87, 3155, 5680, 5781, 7, 140, 81, 27, 5422, 205, 22182, 17854, 18, 476, 3577, 20573, 19, 24, 1283, 203, 977, 6, 48, 47, 1702, 5569, 29, 16587, 5, 11291, 6, 8, 3, 7, 994, 11, 206, 26, 485, 8073, 33, 360, 11, 623, 344, 6, 237, 258, 34, 31, 7, 59, 2538, 114, 128, 2877, 120, 263, 5569, 29, 32, 5, 818, 82, 684, 904, 809, 253, 34, 22565, 6, 16, 2669, 3, 7, 994, 11, 206, 26, 485, 33, 3, 9, 779, 15152, 16, 16531, 10276, 5, 1441, 86, 122, 1635, 5581, 348, 6, 3, 21719, 70, 1525, 12, 207, 625, 4940, 1079, 5222, 6, 141, 3, 7, 994, 8073, 16, 112, 4852, 5, 2, 115, 52, 3, 87, 3155, 2, 115, 52, 3, 87, 3155, 196, 103, 1011, 727, 8, 21075, 7, 21, 8, 685, 24, 136, 3, 7, 994, 2008, 16, 8, 814, 19, 2008, 21, 7235, 3659, 1066, 145, 131, 12, 8700, 151, 11, 143, 540, 12, 36, 2008, 16, 5569, 29, 16587, 8701, 7, 16, 1371, 5, 27, 5422, 205, 22182, 17854, 18, 476, 3577, 20573, 19, 3, 9, 207, 814, 21, 1321, 7570, 12, 810, 8, 3604, 11, 11076, 41, 29, 32, 4930, 3855, 61, 13, 16531, 10276, 5, 299, 310, 6, 48, 814, 744, 31, 17, 43, 231, 13, 3, 9, 5944, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nAttention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nLabels: [2841, 1]\n==================================================\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Step 3 - Setup Training\n\nThe model training process is highly configurable. The [TrainingArguments](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) class effectively exposes the configurable aspects of the process allowing one to customize them accordingly. Here, we will focus on setting up a training process that performs a single epoch of training with a batch size of 16. We will also leverage `adamw_torch` as the optimizer.\n","metadata":{}},{"cell_type":"code","source":"checkpoint_name = \"test-trainer\"\nlocal_checkpoint_path = os.path.join(local_training_root, checkpoint_name)\ntraining_args = tr.TrainingArguments(\n    local_checkpoint_path,\n    num_train_epochs=1,  # default number of epochs to train is 3\n    per_device_train_batch_size=16,\n    optim=\"adamw_torch\",\n    report_to=[\"tensorboard\"],\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:34:50.607015Z","iopub.execute_input":"2024-01-04T21:34:50.607374Z","iopub.status.idle":"2024-01-04T21:34:50.778083Z","shell.execute_reply.started":"2024-01-04T21:34:50.607343Z","shell.execute_reply":"2024-01-04T21:34:50.777310Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"The pre-trained `t5-small` model can be loaded using the [AutoModelForSeq2SeqLM](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForSeq2SeqLM) class.","metadata":{}},{"cell_type":"code","source":"# load the pre-trained model\nmodel = tr.AutoModelForSeq2SeqLM.from_pretrained(\n    model_checkpoint, \n    cache_dir=cache_dir\n)  # Use a pre-cached model\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:34:51.717279Z","iopub.execute_input":"2024-01-04T21:34:51.717703Z","iopub.status.idle":"2024-01-04T21:34:56.207578Z","shell.execute_reply.started":"2024-01-04T21:34:51.717671Z","shell.execute_reply":"2024-01-04T21:34:56.206588Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"779f39a653134ad98037753335e21669"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"041d2318b19148faa1ab9a02edf818e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3de7a75b13243678d9fc56b9dacdb95"}},"metadata":{}}]},{"cell_type":"code","source":"# Used to assist the trainer in batching the data\ndata_collator = tr.DataCollatorWithPadding(tokenizer=tokenizer)\n\ntrainer = tr.Trainer(\n    model,\n    training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:34:56.209502Z","iopub.execute_input":"2024-01-04T21:34:56.209801Z","iopub.status.idle":"2024-01-04T21:35:05.973431Z","shell.execute_reply.started":"2024-01-04T21:34:56.209773Z","shell.execute_reply":"2024-01-04T21:35:05.972637Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":" ### Step 4 - Train\n \n Before starting the training process, let's turn on Tensorboard. This will allow us to monitor the training process as checkpoint logs are created.","metadata":{}},{"cell_type":"code","source":"tensorboard_display_dir = f\"{local_checkpoint_path}/runs\"\n\n%load_ext tensorboard\n%tensorboard --logdir '{tensorboard_display_dir}'\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:45:21.194645Z","iopub.execute_input":"2024-01-04T21:45:21.195363Z","iopub.status.idle":"2024-01-04T21:45:21.207233Z","shell.execute_reply.started":"2024-01-04T21:45:21.195334Z","shell.execute_reply":"2024-01-04T21:45:21.206349Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"The tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Reusing TensorBoard on port 6006 (pid 176), started 0:10:09 ago. (Use '!kill 176' to kill it.)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n      <iframe id=\"tensorboard-frame-3eb13b9046685257\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-3eb13b9046685257\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "},"metadata":{}}]},{"cell_type":"markdown","source":"Start the fine-tuning process.","metadata":{}},{"cell_type":"code","source":"trainer.train()\n\n# save model to the local checkpoint\ntrainer.save_model()\ntrainer.save_state()\n\n# persist the fine-tuned model to DBFS\nfinal_model_path = f\"{cache_dir}/llm04_fine_tuning/{checkpoint_name}\"\ntrainer.save_model(output_dir=final_model_path)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:35:12.524291Z","iopub.execute_input":"2024-01-04T21:35:12.524581Z","iopub.status.idle":"2024-01-04T21:42:59.614626Z","shell.execute_reply.started":"2024-01-04T21:35:12.524555Z","shell.execute_reply":"2024-01-04T21:42:59.613629Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1563/1563 07:44, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.622900</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.139500</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.131400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Step 5 - Predict","metadata":{}},{"cell_type":"code","source":"fine_tuned_model = tr.AutoModelForSeq2SeqLM.from_pretrained(final_model_path)\n\nreviews = [\n    \"\"\"\n    'Despicable Me' is a cute and funny movie, but the plot is predictable and the characters are not very well-developed. Overall, it's a good movie for kids, but adults might find it a bit boring.\n    \"\"\",\n    \"\"\" \n    'The Batman' is a dark and gritty take on the Caped Crusader, starring Robert Pattinson as Bruce Wayne. The film is a well-made crime thriller with strong performances and visuals, but it may be too slow-paced and violent for some viewers.\n    \"\"\",\n    \"\"\"\n    The Phantom Menace is a visually stunning film with some great action sequences, but the plot is slow-paced and the dialogue is often wooden. It is a mixed bag that will appeal to some fans of the Star Wars franchise, but may disappoint others.\n    \"\"\",\n    \"\"\"\n    I'm not sure if The Matrix and the two sequels were meant to have a tigh consistency but I don't think they quite fit together. They seem to have a reasonably solid arc but the features from the first aren't in the second and third as much, instead the second and third focus more on CGI battles and more visuals. I like them but for different reasons, so if I'm supposed to rate the trilogy I'm not sure what to say.\n    \"\"\",\n    \"\"\"\n    I'm not sure if The Matrix and the two sequels were meant to have a tigh consistency but I don't think they quite fit together. They seem to have a reasonably solid arc but the features from the first aren't in the second and third as much, instead the second and third focus more on CGI battles and more visuals. I like them but for different reasons, so if I'm supposed to rate the trilogy I'm not sure what to say.\n    \"\"\",\n    \"\"\"\n    This captivating film delivers a compelling story and outstanding performances, placing it among the best of the year.\n    \"\"\",\n    \"\"\"\n    While the special effects may impress, the lack of a coherent storyline makes this film disappointing for those seeking deeper substance.\n    \"\"\"\n]\n\ninputs = tokenizer(\n    reviews, \n    return_tensors=\"pt\", \n    truncation=True, \n    padding=True\n)\n\npred = fine_tuned_model.generate(\n    input_ids=inputs[\"input_ids\"], \n    attention_mask=inputs[\"attention_mask\"]\n)\n\npdf = pd.DataFrame(\n    zip(reviews, tokenizer.batch_decode(pred, skip_special_tokens=True)),\n    columns=[\"review\", \"classification\"],\n)\ndisplay(pdf)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:44:27.683259Z","iopub.execute_input":"2024-01-04T21:44:27.683992Z","iopub.status.idle":"2024-01-04T21:44:28.234749Z","shell.execute_reply.started":"2024-01-04T21:44:27.683959Z","shell.execute_reply":"2024-01-04T21:44:28.233806Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                                                                                                                                                                                                                                                                                                                                                                                                                          review  \\\n0                                                                                                                                                                                                                                 \\n    'Despicable Me' is a cute and funny movie, but the plot is predictable and the characters are not very well-developed. Overall, it's a good movie for kids, but adults might find it a bit boring.\\n       \n1                                                                                                                                                                                    \\n    'The Batman' is a dark and gritty take on the Caped Crusader, starring Robert Pattinson as Bruce Wayne. The film is a well-made crime thriller with strong performances and visuals, but it may be too slow-paced and violent for some viewers.\\n       \n2                                                                                                                                                                               \\n    The Phantom Menace is a visually stunning film with some great action sequences, but the plot is slow-paced and the dialogue is often wooden. It is a mixed bag that will appeal to some fans of the Star Wars franchise, but may disappoint others.\\n       \n3  \\n    I'm not sure if The Matrix and the two sequels were meant to have a tigh consistency but I don't think they quite fit together. They seem to have a reasonably solid arc but the features from the first aren't in the second and third as much, instead the second and third focus more on CGI battles and more visuals. I like them but for different reasons, so if I'm supposed to rate the trilogy I'm not sure what to say.\\n       \n4  \\n    I'm not sure if The Matrix and the two sequels were meant to have a tigh consistency but I don't think they quite fit together. They seem to have a reasonably solid arc but the features from the first aren't in the second and third as much, instead the second and third focus more on CGI battles and more visuals. I like them but for different reasons, so if I'm supposed to rate the trilogy I'm not sure what to say.\\n       \n5                                                                                                                                                                                                                                                                                                             \\n    This captivating film delivers a compelling story and outstanding performances, placing it among the best of the year.\\n       \n6                                                                                                                                                                                                                                                                                          \\n    While the special effects may impress, the lack of a coherent storyline makes this film disappointing for those seeking deeper substance.\\n       \n\n  classification  \n0       negative  \n1       positive  \n2       positive  \n3       negative  \n4       negative  \n5       positive  \n6       negative  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>classification</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\\n    'Despicable Me' is a cute and funny movie, but the plot is predictable and the characters are not very well-developed. Overall, it's a good movie for kids, but adults might find it a bit boring.\\n</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\\n    'The Batman' is a dark and gritty take on the Caped Crusader, starring Robert Pattinson as Bruce Wayne. The film is a well-made crime thriller with strong performances and visuals, but it may be too slow-paced and violent for some viewers.\\n</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\\n    The Phantom Menace is a visually stunning film with some great action sequences, but the plot is slow-paced and the dialogue is often wooden. It is a mixed bag that will appeal to some fans of the Star Wars franchise, but may disappoint others.\\n</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\\n    I'm not sure if The Matrix and the two sequels were meant to have a tigh consistency but I don't think they quite fit together. They seem to have a reasonably solid arc but the features from the first aren't in the second and third as much, instead the second and third focus more on CGI battles and more visuals. I like them but for different reasons, so if I'm supposed to rate the trilogy I'm not sure what to say.\\n</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\\n    I'm not sure if The Matrix and the two sequels were meant to have a tigh consistency but I don't think they quite fit together. They seem to have a reasonably solid arc but the features from the first aren't in the second and third as much, instead the second and third focus more on CGI battles and more visuals. I like them but for different reasons, so if I'm supposed to rate the trilogy I'm not sure what to say.\\n</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>\\n    This captivating film delivers a compelling story and outstanding performances, placing it among the best of the year.\\n</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>\\n    While the special effects may impress, the lack of a coherent storyline makes this film disappointing for those seeking deeper substance.\\n</td>\n      <td>negative</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}